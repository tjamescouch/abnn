name: "Transformer Basic Test"

layers:
  - type: Input
    name: "input"
    output_shape: [100, 8]

  - type: MultiHeadAttention
    name: "mha_1"
    input_shape: [100, 8]
    output_shape: [100, 8]
    num_heads: 8
    initializer: xavier

  - type: LayerNormalization
    name: "norm_1"
    input_shape: [100, 8]
    output_shape: [100, 8]
    epsilon: 1e-5

  - type: ResidualConnection
    input_shape: [100, 8]
    output_shape: [100, 8]
    name: "residual_connection1"
    from_layer: "input"
    
  - type: Flatten
    name: "flatten"
    input_shape: [100, 8]
    output_size: 800

  - type: Dense
    name: "dense_ffn_expand"
    input_size: 800
    output_size: 1600
    activation: tanh
    initializer: xavier

  - type: Reshape
    name: "reshape"
    input_size: 1600
    output_shape: [100, 16]

  - type: Dropout
    name: "dropout_1"
    rate: 0.1
    input_shape: [100, 16]
    output_shape: [100, 16]

  - type: Flatten
    name: "flatten2"
    input_shape: [100, 16]
    output_size: 1600

  - type: Dense
    name: "dense_ffn_reduce"
    input_size: 1600
    output_size: 800
    activation: linear
    initializer: xavier

  - type: Reshape
    name: "reshape"
    input_size: 800
    output_shape: [100, 8]

  - type: LayerNormalization
    name: "norm_ffn"
    input_shape: [100, 8]
    output_shape: [100, 8]
    epsilon: 1e-5
    
  - type: ResidualConnection
    name: "residual_connection2"
    input_shape: [100, 8]
    output_shape: [100, 8]
    from_layer: "input"
        
  - type: Flatten
    name: "flatten2"
    input_shape: [100, 8]
    output_size: 800

  - type: Dense
    name: "penultimate_dense"
    input_size: 800
    output_size: 800
    activation: tanh
    initializer: xavier
    
  - type: Reshape
    input_size: 800
    output_shape: [100, 8]

  - type: ResidualConnection
    input_shape: [100, 8]
    output_shape: [100, 8]
    from_layer: "norm_1"
    scale: 0.1
    
  - type: LayerNormalization
    name: "post_residual_norm"
    input_shape: [100, 8]
    output_shape: [100, 8]
    epsilon: 1e-5
    
  - type: Flatten
    name: "flatten2"
    input_shape: [100, 8]
    output_size: 800

  - type: Dense
    name: "final_dense"
    input_size: 800
    output_size: 800
    activation: tanh
    initializer: xavier

training:
  optimizer:
    type: adam
    learning_rate: 0.0001
    accumulation_interval: 4
    parameters:
      beta1: 0.9
      beta2: 0.999
      epsilon: 1e-8

  epochs: 15
  batch_size: 2

dataset:
  type: "function"
  dataset_size: 5000

metadata:
  author: "James Couch"
  description: "Basic Transformer with clearly structured Multi-Head Attention, explicit residual connections, and layer normalization."
